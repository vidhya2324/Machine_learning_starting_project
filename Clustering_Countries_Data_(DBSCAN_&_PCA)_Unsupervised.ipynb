{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidhya2324/Machine_learning_starting_project/blob/main/Clustering_Countries_Data_(DBSCAN_%26_PCA)_Unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klzIqdB8V8Cq"
      },
      "source": [
        "Clustering Countries Using Unsupervised Learning for Strategic Aid Allocation by HELP International"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J2SIouNxWKtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95fa543d-18ae-4109-ad08-49c4142d1b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data\n",
            "License(s): MIT\n",
            "Downloading unsupervised-learning-on-country-data.zip to /content\n",
            "  0% 0.00/5.21k [00:00<?, ?B/s]\n",
            "100% 5.21k/5.21k [00:00<00:00, 9.04MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d rohan0301/unsupervised-learning-on-country-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fcu_eEeyWbzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84412329-4057-4715-abee-760d38e78a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  unsupervised-learning-on-country-data.zip\n",
            "  inflating: Country-data.csv        \n",
            "  inflating: data-dictionary.csv     \n"
          ]
        }
      ],
      "source": [
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv5MQdc4Wx6i"
      },
      "source": [
        "#Data Transformation and Feature Engineering\n",
        "\n",
        "Yeo-Johnson Transformation:\n",
        "\n",
        "The PowerTransformer with the **Yeo-Johnson method** is applied to stabilize variance and make the data distribution more Gaussian-like, which can improve the effectiveness of clustering algorithms.\n",
        "\n",
        "Dimensionality Reduction with **PCA**,full form of PCA is principle component Analysis, is used to feature our data into 95%..its can reduce the dimension\n",
        "\n",
        "Clustering with **DBSCAN**\n",
        "\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is selected for its ability to identify clusters of arbitrary shapes and its robustness to noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-Qj-QYsRWhSC"
      },
      "outputs": [],
      "source": [
        "#import the neccessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X-o6kXLOXuRC"
      },
      "outputs": [],
      "source": [
        "# Load the country data from the specified CSV file\n",
        "df = pd.read_csv(\"/content/Country-data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I6LRYNHsX48y"
      },
      "outputs": [],
      "source": [
        "# Define the list of socio-economic and health-related features to be used for clustering\n",
        "features = [\n",
        "    \"child_mort\",          # Child mortality rate\n",
        "    \"exports\",             # Total exports\n",
        "    \"gdpp\",                # Gross Domestic Product per capita\n",
        "    \"health\",              # Health expenditure\n",
        "    \"imports\",             # Total imports\n",
        "    \"income\",              # Average income\n",
        "    \"inflation\",           # Inflation rate\n",
        "    \"life_expec\",          # Life expectancy\n",
        "    \"total_fer\",           # Total fertility rate\n",
        "]\n",
        "# Extract the feature matrix 'X' by selecting the defined features from the DataFrame\n",
        "X = df[features].copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qgrySv9YMCy"
      },
      "source": [
        "#Apply Yeo-Johnson Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TScO4WqvX7Kh"
      },
      "outputs": [],
      "source": [
        "# Initialize the PowerTransformer with the Yeo-Johnson method to stabilize variance and make the data more Gaussian-like\n",
        "pt = PowerTransformer(method=\"yeo-johnson\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JfqAwXKOYNr6"
      },
      "outputs": [],
      "source": [
        "# Fit the transformer to the data and apply the transformation, creating a new DataFrame with transformed features\n",
        "X_transformed = pd.DataFrame(pt.fit_transform(X), columns=X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FcefFDvtY9mi"
      },
      "outputs": [],
      "source": [
        "# Generate interaction terms to capture the combined effect of pairs of features, potentially enhancing clustering performance\n",
        "X_transformed[\"gdpp_health\"] = X_transformed[\"gdpp\"] * X_transformed[\"health\"]\n",
        "X_transformed[\"exports_imports\"] = X_transformed[\"exports\"] * X_transformed[\"imports\"]\n",
        "X_transformed[\"life_expec_child_mort\"] = (\n",
        "    X_transformed[\"life_expec\"] * X_transformed[\"child_mort\"]\n",
        ")\n",
        "X_transformed[\"income_gdpp\"] = X_transformed[\"income\"] * X_transformed[\"gdpp\"]\n",
        "X_transformed[\"health_life_expec\"] = (\n",
        "    X_transformed[\"health\"] * X_transformed[\"life_expec\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Hj3YFCZaNi"
      },
      "source": [
        "#Scale the Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vW-ZrkNHZb5i"
      },
      "outputs": [],
      "source": [
        "# Initialize the RobustScaler to scale features, making them robust to outliers by removing the median and scaling the data according to the quantile range\n",
        "scaler = RobustScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GjanU5avQoeI"
      },
      "outputs": [],
      "source": [
        "# Fit the scaler to the transformed data and apply the scaling\n",
        "X_scaled = scaler.fit_transform(X_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w9jPnTsLQtU_"
      },
      "outputs": [],
      "source": [
        "# Initialize PCA to reduce the dimensionality of the data while retaining 95% of the variance\n",
        "pca = PCA(n_components=0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yCC9mNJf4gub"
      },
      "outputs": [],
      "source": [
        "# Fit PCA on the scaled data and transform it, resulting in a lower-dimensional representation\n",
        "X_pca = pca.fit_transform(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3yDbzBUW7AlE"
      },
      "outputs": [],
      "source": [
        "# Initialize variables to keep track of the best silhouette score and corresponding DBSCAN parameters\n",
        "best_score = -np.inf\n",
        "best_eps = 0\n",
        "best_min_samples = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8KZfd5FJ7GAc"
      },
      "outputs": [],
      "source": [
        "# Define the range of epsilon values to explore for DBSCAN (from 0.1 to 2.0 with step size 0.1)\n",
        "eps_range = np.arange(0.1, 2.1, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Km98XHXC7F9z"
      },
      "outputs": [],
      "source": [
        "# Define the range of min_samples values to explore for DBSCAN (from 2 to 10)\n",
        "min_samples_range = range(2, 11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wT6m1JN47F7G"
      },
      "outputs": [],
      "source": [
        "# Iterate over each combination of epsilon and min_samples\n",
        "for eps in eps_range:\n",
        "    for min_samples in min_samples_range:\n",
        "        # Initialize DBSCAN with the current epsilon and min_samples\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "\n",
        "        # Fit DBSCAN on the PCA-transformed data and predict cluster labels\n",
        "        labels = dbscan.fit_predict(X_pca)\n",
        "        # Calculate the number of clusters found (excluding noise points labeled as -1)\n",
        "        unique_labels = set(labels[labels != -1])\n",
        "        if len(unique_labels) >= 2:\n",
        "            # Create a mask to exclude noise points from the silhouette score calculation\n",
        "            mask = labels != -1\n",
        "            if np.sum(mask) > 1:  # Ensure there are at least two non-noise points\n",
        "                # Calculate the silhouette score for the current clustering\n",
        "                score = silhouette_score(X_pca[mask], labels[mask])\n",
        "\n",
        "                # Update the best score and corresponding parameters if the current score is higher\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_eps = eps\n",
        "                    best_min_samples = min_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FVgOqMIT7ykA"
      },
      "outputs": [],
      "source": [
        "# Check if a valid clustering solution was found\n",
        "if best_score == -np.inf:\n",
        "    # Raise an error if no valid clustering solution was found\n",
        "    raise ValueError(\"No valid clustering solution found\")\n",
        "\n",
        "# Initialize DBSCAN with the best epsilon and min_samples found during grid search\n",
        "final_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
        "# Fit DBSCAN on the PCA-transformed data and predict the final cluster labels\n",
        "final_labels = final_dbscan.fit_predict(X_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RSooGm8D79MX"
      },
      "outputs": [],
      "source": [
        "# Create a mask to exclude noise points from the silhouette score calculation\n",
        "mask = final_labels != -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VYU3V_XS79I8"
      },
      "outputs": [],
      "source": [
        "# Calculate the silhouette score for the final clustering\n",
        "final_score = silhouette_score(X_pca[mask], final_labels[mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSs1iyWs8GGi",
        "outputId": "986bcb8a-09d7-4e46-df75-65f3dd30b822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: eps=0.30, min_samples=2\n",
            "Number of clusters: 2\n",
            "Silhouette Score: 0.9285\n",
            "Number of noise points: 163\n",
            "Number of PCA components: 7\n"
          ]
        }
      ],
      "source": [
        "# Print the best DBSCAN parameters found\n",
        "print(f\"Best Parameters: eps={best_eps:.2f}, min_samples={best_min_samples}\")\n",
        "\n",
        "# Print the number of clusters formed with the best parameters\n",
        "print(f\"Number of clusters: {len(set(final_labels[final_labels != -1]))}\")\n",
        "\n",
        "# Print the silhouette score of the final clustering\n",
        "print(f\"Silhouette Score: {final_score:.4f}\")\n",
        "\n",
        "# Print the number of noise points identified by DBSCAN\n",
        "print(f\"Number of noise points: {np.sum(final_labels == -1)}\")\n",
        "\n",
        "# Print the number of PCA components used\n",
        "print(f\"Number of PCA components: {X_pca.shape[1]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyuaQEkL3PT3zXyoXzR6Uh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}