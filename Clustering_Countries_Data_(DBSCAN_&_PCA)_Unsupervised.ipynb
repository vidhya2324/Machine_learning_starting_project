{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidhya2324/Machine_learning_starting_project/blob/main/Clustering_Countries_Data_(DBSCAN_%26_PCA)_Unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klzIqdB8V8Cq"
      },
      "source": [
        "Clustering Countries Using Unsupervised Learning for Strategic Aid Allocation by HELP International"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J2SIouNxWKtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95fa543d-18ae-4109-ad08-49c4142d1b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data\n",
            "License(s): MIT\n",
            "Downloading unsupervised-learning-on-country-data.zip to /content\n",
            "  0% 0.00/5.21k [00:00<?, ?B/s]\n",
            "100% 5.21k/5.21k [00:00<00:00, 9.04MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d rohan0301/unsupervised-learning-on-country-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fcu_eEeyWbzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84412329-4057-4715-abee-760d38e78a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  unsupervised-learning-on-country-data.zip\n",
            "  inflating: Country-data.csv        \n",
            "  inflating: data-dictionary.csv     \n"
          ]
        }
      ],
      "source": [
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv5MQdc4Wx6i"
      },
      "source": [
        "#Data Transformation and Feature Engineering\n",
        "\n",
        "Yeo-Johnson Transformation:\n",
        "\n",
        "The PowerTransformer with the **Yeo-Johnson method** is applied to stabilize variance and make the data distribution more Gaussian-like, which can improve the effectiveness of clustering algorithms.\n",
        "\n",
        "Dimensionality Reduction with **PCA**,full form of PCA is principle component Analysis, is used to feature our data into 95%..its can reduce the dimension\n",
        "\n",
        "Clustering with **DBSCAN**\n",
        "\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is selected for its ability to identify clusters of arbitrary shapes and its robustness to noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-Qj-QYsRWhSC"
      },
      "outputs": [],
      "source": [
        "#import the neccessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X-o6kXLOXuRC"
      },
      "outputs": [],
      "source": [
        "# Load the country data from the specified CSV file\n",
        "df = pd.read_csv(\"/content/Country-data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I6LRYNHsX48y"
      },
      "outputs": [],
      "source": [
        "# Define the list of socio-economic and health-related features to be used for clustering\n",
        "features = [\n",
        "    \"child_mort\",          # Child mortality rate\n",
        "    \"exports\",             # Total exports\n",
        "    \"gdpp\",                # Gross Domestic Product per capita\n",
        "    \"health\",              # Health expenditure\n",
        "    \"imports\",             # Total imports\n",
        "    \"income\",              # Average income\n",
        "    \"inflation\",           # Inflation rate\n",
        "    \"life_expec\",          # Life expectancy\n",
        "    \"total_fer\",           # Total fertility rate\n",
        "]\n",
        "# Extract the feature matrix 'X' by selecting the defined features from the DataFrame\n",
        "X = df[features].copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qgrySv9YMCy"
      },
      "source": [
        "#Apply Yeo-Johnson Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TScO4WqvX7Kh"
      },
      "outputs": [],
      "source": [
        "# Initialize the PowerTransformer with the Yeo-Johnson method to stabilize variance and make the data more Gaussian-like\n",
        "pt = PowerTransformer(method=\"yeo-johnson\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JfqAwXKOYNr6"
      },
      "outputs": [],
      "source": [
        "# Fit the transformer to the data and apply the transformation, creating a new DataFrame with transformed features\n",
        "X_transformed = pd.DataFrame(pt.fit_transform(X), columns=X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FcefFDvtY9mi"
      },
      "outputs": [],
      "source": [
        "# Generate interaction terms to capture the combined effect of pairs of features, potentially enhancing clustering performance\n",
        "X_transformed[\"gdpp_health\"] = X_transformed[\"gdpp\"] * X_transformed[\"health\"]\n",
        "X_transformed[\"exports_imports\"] = X_transformed[\"exports\"] * X_transformed[\"imports\"]\n",
        "X_transformed[\"life_expec_child_mort\"] = (\n",
        "    X_transformed[\"life_expec\"] * X_transformed[\"child_mort\"]\n",
        ")\n",
        "X_transformed[\"income_gdpp\"] = X_transformed[\"income\"] * X_transformed[\"gdpp\"]\n",
        "X_transformed[\"health_life_expec\"] = (\n",
        "    X_transformed[\"health\"] * X_transformed[\"life_expec\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Hj3YFCZaNi"
      },
      "source": [
        "#Scale the Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vW-ZrkNHZb5i"
      },
      "outputs": [],
      "source": [
        "# Initialize the RobustScaler to scale features, making them robust to outliers by removing the median and scaling the data according to the quantile range\n",
        "scaler = RobustScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GjanU5avQoeI"
      },
      "outputs": [],
      "source": [
        "# Fit the scaler to the transformed data and apply the scaling\n",
        "X_scaled = scaler.fit_transform(X_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w9jPnTsLQtU_"
      },
      "outputs": [],
      "source": [
        "# Initialize PCA to reduce the dimensionality of the data while retaining 95% of the variance\n",
        "pca = PCA(n_components=0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yCC9mNJf4gub"
      },
      "outputs": [],
      "source": [
        "# Fit PCA on the scaled data and transform it, resulting in a lower-dimensional representation\n",
        "X_pca = pca.fit_transform(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3yDbzBUW7AlE"
      },
      "outputs": [],
      "source": [
        "# Initialize variables to keep track of the best silhouette score and corresponding DBSCAN parameters\n",
        "best_score = -np.inf\n",
        "best_eps = 0\n",
        "best_min_samples = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8KZfd5FJ7GAc"
      },
      "outputs": [],
      "source": [
        "# Define the range of epsilon values to explore for DBSCAN (from 0.1 to 2.0 with step size 0.1)\n",
        "eps_range = np.arange(0.1, 2.1, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Km98XHXC7F9z"
      },
      "outputs": [],
      "source": [
        "# Define the range of min_samples values to explore for DBSCAN (from 2 to 10)\n",
        "min_samples_range = range(2, 11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wT6m1JN47F7G"
      },
      "outputs": [],
      "source": [
        "# Iterate over each combination of epsilon and min_samples\n",
        "for eps in eps_range:\n",
        "    for min_samples in min_samples_range:\n",
        "        # Initialize DBSCAN with the current epsilon and min_samples\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "\n",
        "        # Fit DBSCAN on the PCA-transformed data and predict cluster labels\n",
        "        labels = dbscan.fit_predict(X_pca)\n",
        "        # Calculate the number of clusters found (excluding noise points labeled as -1)\n",
        "        unique_labels = set(labels[labels != -1])\n",
        "        if len(unique_labels) >= 2:\n",
        "            # Create a mask to exclude noise points from the silhouette score calculation\n",
        "            mask = labels != -1\n",
        "            if np.sum(mask) > 1:  # Ensure there are at least two non-noise points\n",
        "                # Calculate the silhouette score for the current clustering\n",
        "                score = silhouette_score(X_pca[mask], labels[mask])\n",
        "\n",
        "                # Update the best score and corresponding parameters if the current score is higher\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_eps = eps\n",
        "                    best_min_samples = min_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FVgOqMIT7ykA"
      },
      "outputs": [],
      "source": [
        "# Check if a valid clustering solution was found\n",
        "if best_score == -np.inf:\n",
        "    # Raise an error if no valid clustering solution was found\n",
        "    raise ValueError(\"No valid clustering solution found\")\n",
        "\n",
        "# Initialize DBSCAN with the best epsilon and min_samples found during grid search\n",
        "final_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
        "# Fit DBSCAN on the PCA-transformed data and predict the final cluster labels\n",
        "final_labels = final_dbscan.fit_predict(X_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RSooGm8D79MX"
      },
      "outputs": [],
      "source": [
        "# Create a mask to exclude noise points from the silhouette score calculation\n",
        "mask = final_labels != -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VYU3V_XS79I8"
      },
      "outputs": [],
      "source": [
        "# Calculate the silhouette score for the final clustering\n",
        "final_score = silhouette_score(X_pca[mask], final_labels[mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSs1iyWs8GGi",
        "outputId": "986bcb8a-09d7-4e46-df75-65f3dd30b822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: eps=0.30, min_samples=2\n",
            "Number of clusters: 2\n",
            "Silhouette Score: 0.9285\n",
            "Number of noise points: 163\n",
            "Number of PCA components: 7\n"
          ]
        }
      ],
      "source": [
        "# Print the best DBSCAN parameters found\n",
        "print(f\"Best Parameters: eps={best_eps:.2f}, min_samples={best_min_samples}\")\n",
        "\n",
        "# Print the number of clusters formed with the best parameters\n",
        "print(f\"Number of clusters: {len(set(final_labels[final_labels != -1]))}\")\n",
        "\n",
        "# Print the silhouette score of the final clustering\n",
        "print(f\"Silhouette Score: {final_score:.4f}\")\n",
        "\n",
        "# Print the number of noise points identified by DBSCAN\n",
        "print(f\"Number of noise points: {np.sum(final_labels == -1)}\")\n",
        "\n",
        "# Print the number of PCA components used\n",
        "print(f\"Number of PCA components: {X_pca.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#real time code building\n"
      ],
      "metadata": {
        "id": "Z8lXBTGhz19L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Load the country data\n",
        "df = pd.read_csv(\"/content/Country-data.csv\")\n",
        "\n",
        "# Features for clustering\n",
        "features = [\n",
        "    \"child_mort\", \"exports\", \"gdpp\", \"health\",\n",
        "    \"imports\", \"income\", \"inflation\",\n",
        "    \"life_expec\", \"total_fer\"\n",
        "]\n",
        "X = df[features].copy()\n",
        "\n",
        "# Preprocessing: Yeo-Johnson Transformation\n",
        "pt = PowerTransformer(method=\"yeo-johnson\")\n",
        "X_transformed = pd.DataFrame(pt.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Add interaction terms\n",
        "X_transformed[\"gdpp_health\"] = X_transformed[\"gdpp\"] * X_transformed[\"health\"]\n",
        "X_transformed[\"exports_imports\"] = X_transformed[\"exports\"] * X_transformed[\"imports\"]\n",
        "X_transformed[\"life_expec_child_mort\"] = X_transformed[\"life_expec\"] * X_transformed[\"child_mort\"]\n",
        "X_transformed[\"income_gdpp\"] = X_transformed[\"income\"] * X_transformed[\"gdpp\"]\n",
        "X_transformed[\"health_life_expec\"] = X_transformed[\"health\"] * X_transformed[\"life_expec\"]\n",
        "\n",
        "# Scaling and PCA\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X_transformed)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Clustering using DBSCAN\n",
        "best_eps = 0.30  # Use optimal values from previous training\n",
        "best_min_samples = 2\n",
        "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
        "final_labels = dbscan.fit_predict(X_pca)\n",
        "\n",
        "# Adding cluster labels to the dataframe\n",
        "df['Cluster'] = final_labels\n",
        "\n",
        "# Function to suggest a country for aid allocation\n",
        "def suggest_countries(df, cluster_priority=0):\n",
        "    \"\"\"\n",
        "    Suggest countries for aid allocation based on cluster priority.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Dataframe with country data and cluster labels.\n",
        "        cluster_priority (int): Cluster number to prioritize for aid.\n",
        "\n",
        "    Returns:\n",
        "        list: List of countries in the prioritized cluster.\n",
        "    \"\"\"\n",
        "    # Filter countries in the specified cluster\n",
        "    target_countries = df[df['Cluster'] == cluster_priority]['country'].tolist()\n",
        "    return target_countries\n",
        "\n",
        "# Example: Get countries in cluster 0 (most needy cluster)\n",
        "priority_countries = suggest_countries(df, cluster_priority=0)\n",
        "\n",
        "# Display suggested countries\n",
        "print(\"Suggested countries for aid allocation:\")\n",
        "print(priority_countries)\n",
        "\n",
        "# Optional: Save the output to a CSV for record-keeping\n",
        "df.to_csv(\"clustered_countries.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxO-AooOx0Ga",
        "outputId": "4609282b-6279-48de-adcf-71f6a2f7994f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggested countries for aid allocation:\n",
            "['Croatia', 'Poland']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Load the country data\n",
        "df = pd.read_csv(\"/content/Country-data.csv\")\n",
        "\n",
        "# Features for clustering\n",
        "features = [\n",
        "    \"child_mort\", \"exports\", \"gdpp\", \"health\",\n",
        "    \"imports\", \"income\", \"inflation\",\n",
        "    \"life_expec\", \"total_fer\"\n",
        "]\n",
        "X = df[features].copy()\n",
        "\n",
        "# Preprocessing: Yeo-Johnson Transformation\n",
        "pt = PowerTransformer(method=\"yeo-johnson\")\n",
        "X_transformed = pd.DataFrame(pt.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Add interaction terms\n",
        "X_transformed[\"gdpp_health\"] = X_transformed[\"gdpp\"] * X_transformed[\"health\"]\n",
        "X_transformed[\"exports_imports\"] = X_transformed[\"exports\"] * X_transformed[\"imports\"]\n",
        "X_transformed[\"life_expec_child_mort\"] = X_transformed[\"life_expec\"] * X_transformed[\"child_mort\"]\n",
        "X_transformed[\"income_gdpp\"] = X_transformed[\"income\"] * X_transformed[\"gdpp\"]\n",
        "X_transformed[\"health_life_expec\"] = X_transformed[\"health\"] * X_transformed[\"life_expec\"]\n",
        "\n",
        "# Scaling and PCA\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X_transformed)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Clustering using DBSCAN\n",
        "best_eps = 0.30  # Use optimal values from previous training\n",
        "best_min_samples = 2\n",
        "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
        "final_labels = dbscan.fit_predict(X_pca)\n",
        "\n",
        "# Adding cluster labels to the dataframe\n",
        "df['Cluster'] = final_labels\n",
        "\n",
        "# Prepare transformation pipelines\n",
        "def preprocess_user_input(user_input):\n",
        "    \"\"\"\n",
        "    Preprocess the user input using the trained pipelines (PowerTransformer, RobustScaler, PCA).\n",
        "\n",
        "    Args:\n",
        "        user_input (dict): Dictionary of user-provided socio-economic data.\n",
        "\n",
        "    Returns:\n",
        "        np.array: PCA-transformed user input.\n",
        "    \"\"\"\n",
        "    input_df = pd.DataFrame([user_input], columns=features)\n",
        "\n",
        "    # Apply Yeo-Johnson transformation\n",
        "    input_transformed = pd.DataFrame(pt.transform(input_df), columns=input_df.columns)\n",
        "\n",
        "    # Add interaction terms\n",
        "    input_transformed[\"gdpp_health\"] = input_transformed[\"gdpp\"] * input_transformed[\"health\"]\n",
        "    input_transformed[\"exports_imports\"] = input_transformed[\"exports\"] * input_transformed[\"imports\"]\n",
        "    input_transformed[\"life_expec_child_mort\"] = input_transformed[\"life_expec\"] * input_transformed[\"child_mort\"]\n",
        "    input_transformed[\"income_gdpp\"] = input_transformed[\"income\"] * input_transformed[\"gdpp\"]\n",
        "    input_transformed[\"health_life_expec\"] = input_transformed[\"health\"] * input_transformed[\"life_expec\"]\n",
        "\n",
        "    # Apply scaling and PCA\n",
        "    scaled_input = scaler.transform(input_transformed)\n",
        "    pca_input = pca.transform(scaled_input)\n",
        "\n",
        "    return pca_input\n",
        "\n",
        "# Suggest a cluster for a new country\n",
        "def suggest_cluster(user_input):\n",
        "    \"\"\"\n",
        "    Suggest the cluster for the user input and determine if aid is recommended.\n",
        "\n",
        "    Args:\n",
        "        user_input (dict): Dictionary of socio-economic metrics for a country.\n",
        "\n",
        "    Returns:\n",
        "        int: Cluster label for the input country.\n",
        "    \"\"\"\n",
        "    processed_input = preprocess_user_input(user_input)\n",
        "    cluster_label = dbscan.fit_predict(processed_input)[0]\n",
        "    return cluster_label\n",
        "\n",
        "# Real-time user input\n",
        "print(\"Provide the socio-economic metrics for the country you want to analyze.\")\n",
        "user_input = {\n",
        "    \"child_mort\": float(input(\"Child mortality rate: \")),\n",
        "    \"exports\": float(input(\"Total exports (% of GDP): \")),\n",
        "    \"gdpp\": float(input(\"GDP per capita: \")),\n",
        "    \"health\": float(input(\"Health expenditure (% of GDP): \")),\n",
        "    \"imports\": float(input(\"Total imports (% of GDP): \")),\n",
        "    \"income\": float(input(\"Average income: \")),\n",
        "    \"inflation\": float(input(\"Inflation rate: \")),\n",
        "    \"life_expec\": float(input(\"Life expectancy: \")),\n",
        "    \"total_fer\": float(input(\"Total fertility rate: \"))\n",
        "}\n",
        "\n",
        "# Determine the cluster for the user input\n",
        "cluster = suggest_cluster(user_input)\n",
        "\n",
        "# Output the cluster label and decision\n",
        "if cluster == -1:\n",
        "    print(\"The input country is classified as noise, which means it doesn't fit into any defined cluster.\")\n",
        "else:\n",
        "    print(f\"The input country belongs to cluster {cluster}.\")\n",
        "    if cluster == 0:  # Assuming cluster 0 is the priority for aid\n",
        "        print(\"Aid allocation is recommended for this country.\")\n",
        "    else:\n",
        "        print(\"Aid allocation may not be necessary for this country.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_o14mOmyJCs",
        "outputId": "be96db32-9903-4b08-e53e-74d0e6f135fd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Provide the socio-economic metrics for the country you want to analyze.\n",
            "Child mortality rate: 25\n",
            "Total exports (% of GDP): 30\n",
            "GDP per capita: 1500\n",
            "Health expenditure (% of GDP): 4\n",
            "Total imports (% of GDP): 40\n",
            "Average income: 2500\n",
            "Inflation rate: 3\n",
            "Life expectancy: 15\n",
            "Total fertility rate: 3\n",
            "The input country is classified as noise, which means it doesn't fit into any defined cluster.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Cluster\"] = final_labels\n",
        "print(final_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyZJt7jWvNWB",
        "outputId": "127e1d34-363d-4972-a63e-4ee7eb58fe4d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_0_countries = df[df[\"Cluster\"] == 0]\n",
        "cluster_1_countries = df[df[\"Cluster\"] == 1]\n"
      ],
      "metadata": {
        "id": "lKcgTjfCvVCU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cluster_0_countries)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Somp8nkYvhJW",
        "outputId": "370d7d98-c434-43bc-88b5-ef7cf6c92559"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     country  child_mort  exports  health  imports  income  inflation  \\\n",
            "41   Croatia         5.5     37.6    7.76     38.1   20100      0.821   \n",
            "121   Poland         6.0     40.1    7.46     42.1   21800      1.660   \n",
            "\n",
            "     life_expec  total_fer   gdpp  Cluster  \n",
            "41         76.3       1.55  13500        0  \n",
            "121        76.3       1.41  12600        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noise_countries = df[df[\"Cluster\"] == -1]"
      ],
      "metadata": {
        "id": "_pH_RdjMvmEN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Countries in Cluster 0:\")\n",
        "print(cluster_0_countries[\"country\"].tolist())\n",
        "\n",
        "print(\"Countries in Cluster 1:\")\n",
        "print(cluster_1_countries[\"country\"].tolist())\n",
        "\n",
        "print(\"Noise Countries:\")\n",
        "print(noise_countries[\"country\"].tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddHFqFGMvuLL",
        "outputId": "dbf08fe1-383b-4cce-92c5-e07f2d9d12fe"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Countries in Cluster 0:\n",
            "['Croatia', 'Poland']\n",
            "Countries in Cluster 1:\n",
            "['Italy', 'Spain']\n",
            "Noise Countries:\n",
            "['Afghanistan', 'Albania', 'Algeria', 'Angola', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Congo, Dem. Rep.', 'Congo, Rep.', 'Costa Rica', \"Cote d'Ivoire\", 'Cyprus', 'Czech Republic', 'Denmark', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Fiji', 'Finland', 'France', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyz Republic', 'Lao', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Lithuania', 'Luxembourg', 'Macedonia, FYR', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Mauritania', 'Mauritius', 'Micronesia, Fed. Sts.', 'Moldova', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Paraguay', 'Peru', 'Philippines', 'Portugal', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'Samoa', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovak Republic', 'Slovenia', 'Solomon Islands', 'South Africa', 'South Korea', 'Sri Lanka', 'St. Vincent and the Grenadines', 'Sudan', 'Suriname', 'Sweden', 'Switzerland', 'Tajikistan', 'Tanzania', 'Thailand', 'Timor-Leste', 'Togo', 'Tonga', 'Tunisia', 'Turkey', 'Turkmenistan', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Vietnam', 'Yemen', 'Zambia']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7l1wMeW/dZVujnSYlk99N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}